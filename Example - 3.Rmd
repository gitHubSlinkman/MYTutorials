---
title: "5 Spending the data"
author: "by Mel Moreno and Mathieu Basille interpreted by Craig Slinkman"
date: "10/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

There are several steps to create a useful model, including parameter estimation, model selection and tuning, and performance assessment. At the start of a new project, there is usually an initial finite pool of data available for all these tasks. How should the data be applied to these steps? The idea of data spending is an important first consideration when modeling, especially as it relates to empirical validation.

When there are copious amounts of data available, a smart strategy is to allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only. There may be questions about many modeling project steps that must be answered with limited prior knowledge. For example, one possible strategy (when both data and predictors are abundant) is to spend a specific subset of data to determine which predictors are informative, before considering parameter estimation at all.

>Author's remarks:  As data are reused for multiple tasks, certain risks increase, such as the risks of adding bias or large effects from methodological errors.

## 5.1 Common methods for splitting data

The primary approach for empirical model validation is to split the existing pool of data into two distinct sets. Some observations are used to develop and optimize the model. This *training set* is usually the majority of the data. These data are a sandbox for model building where different models can be fit, feature engineering strategies are investigated, and so on. We as modeling practitioners spend the vast majority of the modeling process using the training set as the substrate to develop the model.

The other portion of the observations are placed into the *test set*. This is held in reserve until one or two models are chosen as the methods that are mostly likely to succeed. The test set is then used as the final arbiter to determine the efficacy of the model. It is critical to only look at the test set once; otherwise, it becomes part of the modeling process.  

>Author's remark: How should we conduct this split of the data? This depends on the context.  

Suppose we allocate 80% of the data to the training set and the remaining 20% for testing. The most common method is to use simple random sampling. The **rsample** package has tools for making data splits such as this; the function **intial_split()** was created for this purpose. It takes the data frame as an argument as well as the proportion to be placed into training.

Let's use the Ames data:

```{r}
library(tidymodels)                     # The object of this tutorial
library( flextable )                    # For table output.

data(ames)                              # Get our data.
ames                             


################################################################################
# Recall that we need to transform the Sale_Price because pur outcome variable
# is positively skewed.
################################################################################

ames <-
  ames %>% 
    mutate( Sale_Price <- log10( Sale_Price ))




###############################################################################
# Set the random number stream using `set.seed()` so that the results can be 
# reproduced later.
###############################################################################

set.seed(123)


################################################################################
# Save the split information for an 80/20 split of the data.
################################################################################

ames_split <- ames_split <- initial_split(ames, prob = 0.80)
ames_split
```  

he printed information denotes the amount of data in the training set (n=2,198), the amount in the test set (n=732), and the size of the original pool of samples (n=2,930

).

The object ames_split is an rsplit object and only contains the partitioning information; to get the resulting data sets, we apply two more functions:

```{r}
ames_train <- training(ames_split)
dim( ames_train )[1]

ames_test  <-  testing(ames_split)
dim( ames_test )
```  

Simple random sampling is appropriate in many cases but there are exceptions. When there is a dramatic class imbalance in classification problems, one class occurs much less frequently than another. Using a simple random sample may haphazardly allocate these infrequent samples disproportionately into the training or test set. To avoid this, stratified sampling can be used. The training/test split is conducted separately within each class and then these subsamples are combined into the overall training and test set. For regression problems, the outcome data can be artificially binned into quartiles and then stratified sampling conducted four separate times. This is an effective method for keeping the distributions of the outcome similar between the training and test set.

For the logged Ames data we compute the first, second, and third quartiles and plot them on the density histogram as follows:

```{r}
q1 <- quantile( ames$log_Sale_Price, 0.25 )
q2 <- quantile( ames$log_Sale_Price, 0.5 )
q3 <- quantile( ames$log_Sale_Price , 0.75 )
quartiles <- c( q1, q2, q3 )
labels <- c( "First quartile", "Median", "Third Quartile")
tibble( labels, quartiles ) %>%
  flextable()
```

